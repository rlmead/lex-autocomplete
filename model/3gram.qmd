---
title: "3gram Model"
author: "Josef Fruehwald"
date: today
editor: visual
license: MIT
format:
  html:
    embed-resources: true
    theme: darkly
---

## Setup

I'm going to try to keep the install size small, so rather than installing the whole tidyverse metapackage, I'll just install the specific libraries I use.

Requires `renv` and `reticulate`.

```{r}
#| label: install
#| eval: false
# ensure renv is activated
# source(".Rprofile)
renv::refresh()
```

```{r}
library(dplyr)
library(readr)
library(stringr)
library(purrr)
library(tidyr)
library(tidytext)
library(hunspell)
```

```{r}
library(spacyr)
```

You have to sidestep spacyr's defaults for python versions

```{bash}
#| eval: false
#| filename: "bash"
conda create --yes --name spacy_condaenv python=3.8 spacy -c conda-forge

# only if you need to unstall rust
#curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
```

```{r}
#| eval: false
#spacy_install(lang_models = "en_core_web_trf")
```

```{r}
spacy_initialize(model = "en_core_web_trf")
```

## Load Raw Data

```{r}
dat <- read_csv("../data/Codes Summed-Main View.csv") %>%
          rename_with(~tolower(gsub("\\s", "_", .x)))
```

Checking to ensure there is a unique value of `Response` for every row.

```{r}
nrow(dat) == length(unique(dat$response))
nrow(dat)
```

Focusing in on the qualitative comments.

```{r}
dat %>%
  select(response, qualitative_response) %>%
  mutate(qualitative_response = str_replace_all(qualitative_response, "\\s+", " "))-> corpus
```

```{r}
#' Pad out a vector
#' 
#' This pads out a vector on both ends with start and end symbols
#' @param x the input vector
#' @param n the size of the left and right padding
#' @param start_symbol the start pad symbol
#' @param end_symbol the end pad symbol
#' @returns A vector \code{(2*n) + len(x)} long
pad_vector <- function(x, n = 2, start_symbol = "<s>", end_symbol = "</s>"){
  start_pad = rep(start_symbol, times = n)
  end_pad = rep(end_symbol, times = n)
  out <- c(start_pad, x, end_pad)
  return(out)
}

```

```{r}
Sys.setenv(TOKENIZERS_PARALLELISM="false")
comment_parse <- spacy_parse(corpus$qualitative_response)
```

```{r}
pad_df <- function(df, n = 2, start_symbol = "<s>", end_symbol = "</s>"){
  start_pad <- tibble(token = rep(start_symbol, times = n))
  end_pad <- tibble(token = rep(end_symbol, times = n))
  out <- bind_rows(start_pad, df, end_pad) %>%
          replace_na(list(sentence_id = 0, token_id = 0, pos = '', 
                          entity_type = '', dir = '')) %>%
          mutate(lemma = token)
  return(out)
}
```

```{r}
types_to_consolidate <- c("EVENT", "FAC", "GPE", "LANGUAGE", "LAW", "LOC", "NORP",
                          "ORG", "PERSON", "PRODUCT", "WORK_OF_ART")
```

```{r}
comment_parse %>%
  mutate(token = case_when(entity != '' & token == "Ave" ~ "Avenue",
                           entity != '' & token == "Pkwy" ~ "Parkway",
                           entity != '' & token == "Rd" ~ "Road",
                           entity != '' & token == "rd" ~ "Road",
                           entity != '' & token == "St." ~ "Street" ,
                           TRUE ~ token)) %>%
  entity_consolidate(concatenator = " ") %>%
  mutate(token = case_when(entity_type != '' & 
                             str_detect(token, 
                                        "^[Ll][Ee][Xx]\\W*[Tt][Rr][Aa][Nn]$") ~ "Lextran",
                           entity_type %in% types_to_consolidate &
                             !str_detect(token, "^[:upper:]+$") ~ str_to_title(token),
                           pos == "PRON" & str_detect(token, "^[Ii]$") ~ "I",
                           TRUE ~ tolower(token)))->consolidated
```

```{r}
# consolidated %>%
#   filter(str_detect(token, "'\\w+?")) %>%
#   count(token) %>%
#   arrange(desc(n)) %>%
#   slice(1:7) %>%
#   select(-n) %>%
#   write_csv(file = "../data/00_leaners.csv")
```

```{r}
# consolidated %>%
#   filter(pos == "PUNCT") %>%
#   count(token) %>%
#   arrange(desc(n)) %>%
#   select(-n) %>%
#   write_csv("../data/01_leaners.csv")
```

```{r}
# consolidated %>%
#   filter(str_detect(token, "’\\w+?")) %>%
#   count(token) %>%
#   arrange(desc(n)) %>%
#   select(-n) %>%
#   mutate(dir = "<") %>%
#   slice(1:7) %>%
#   write_csv(file = "../data/02_leaners.csv")
```

```{r}
lean_df <- map_dfr(list.files(path = "../data/", pattern = "*_leaners.csv", full.names = T), read_csv)
```

```{r}
consolidated %>%
  mutate(id = 1:n())%>%
  left_join(lean_df) %>%
  replace_na(list(dir = "")) %>%
  mutate(token = str_replace_all(token, "[’‘]", "'"),
         token = str_replace_all(token, "[“”]", '"')) %>%
  arrange(id) %>%
  group_by(doc_id) %>%
  nest() %>%
  mutate(padded = map(data, pad_df)) -> pad_nest
```

```{r}
pad_nest %>%
  select(-data) %>%
  unnest(padded) %>%
  select(doc_id, token, pos, entity_type, dir) %>%
  mutate(word_i = lag(token, 2),
         word_j = lag(token, 1),
         word_k = token)  -> trigrams
```

```{r}
trigrams %>%
  ungroup() %>%
  count(word_i, word_j) %>%
  rename(bigram_count = n)-> bigram_count
```

```{r}
trigrams %>%
 ungroup() %>%
 count(word_i, word_j, word_k, dir) %>%
 rename(trigram_count = n) -> trigram_count
```

```{r}
trigram_count %>%
  left_join(bigram_count) %>%
  mutate(cond_prob = trigram_count/bigram_count)->model
```

```{r}
model %>%
  filter(word_i == "watersheds", word_j == ".")
```

```{r}
generate <- function(df, w_i = "<s>", w_j = "<s>", max = 10000){
  out <- vector()
  x = 0
  stop = FALSE
  while(x < max & !stop){
    sample = df %>% 
                filter(word_i == w_i, word_j == w_j) %>% 
                slice_sample(n = 1, weight_by = cond_prob)
    if(sample$word_k == "</s>") {
      stop = TRUE
      break
    }
    if(length(out) == 0){
      paste_word = str_to_title(sample$word_k)
    }else if (w_j == "."){
      paste_word = str_to_title(sample$word_k)
    }else{
      paste_word = sample$word_k
    }
    left_side <- NULL
    if(length(out) > 0){
      if(rev(out)[1] == ""){
        left_side <- NULL
      }else if(str_detect(sample$dir, "<")){
        left_side <- NULL
      }else{
        left_side <- " "
      }
    }
    if(str_detect(sample$dir, ">")){
      right_side <- ""
    }else{
      right_side <- NULL
    }
    out <- c(out, left_side, paste_word, right_side)
    w_i = w_j
    w_j = sample$word_k
    x = x+1
  }
  return(str_c(out, collapse = ""))
}
```

```{r}
generate(model)
```
